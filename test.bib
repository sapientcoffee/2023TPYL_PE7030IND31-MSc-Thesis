@article{Hart1988,
   abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload. © 1988 Elsevier Science & Technology.},
   author = {Sandra G. Hart and Lowell E. Staveland},
   doi = {10.1016/S0166-4115(08)62386-9},
   issn = {0166-4115},
   issue = {C},
   journal = {Advances in Psychology},
   month = {1},
   pages = {139-183},
   publisher = {North-Holland},
   title = {Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research},
   volume = {52},
   year = {1988},
}
@misc{,
   title = {State of DevOps Report 2023},
   year = {2023},
}
@article{Wickens1984,
   author = {C Wickens},
   journal = {R. Parasuraman and D. Davis, Eds. Academic Press},
   title = {Processing resources and attention, varieties of attention},
   year = {1984},
}
@inbook{Wickens2020,
   author = {Christopher D Wickens},
   journal = {Multiple task performance},
   pages = {3-34},
   publisher = {CRC Press},
   title = {Processing resources and attention},
   year = {2020},
}
@article{Wickens2008,
   author = {Christopher D Wickens},
   issn = {0018-7208},
   issue = {3},
   journal = {Human factors},
   pages = {449-455},
   publisher = {SAGE Publications Sage CA: Los Angeles, CA},
   title = {Multiple resources and mental workload},
   volume = {50},
   year = {2008},
}
@article{Paxion2014,
   abstract = {L'objectif de cette thèse est d'identifier les liens entre les facteurs accidentogènes chez les jeunes conducteurs, en analysant les effets de la complexité de la situation et de l'expérience de conduite sur la charge de travail, la tension, la vigilance et les performances. L'hypothèse générale est que les situations coûteuses, i.e. simples et très complexes, ainsi que le manque d'expérience peuvent mener à une forte dépense énergétique. Ainsi, un haut niveau de tension et un faible niveau de vigilance peuvent provoquer des dégradations de performances à travers la hausse de la charge de travail, caractérisant une surcharge du conducteur. Ceci devrait s'observer plus précocement pour les novices ayant suivi un Apprentissage Traditionnel (AT), puis pour ceux ayant suivi un Apprentissage Anticipé de la Conduite (AAC), puis pour les conducteurs arrivant à la fin du permis probatoire, et enfin pour les plus expérimentés. Une première expérimentation menée sur des données subjectives a confirmé les liens entre les niveaux de charge de travail, de tension et de vigilance en fonction de la complexité de la situation. Cependant, seuls les novices AAC étaient surchargés avec la complexité de la situation, de part des stratégies inefficaces. Une seconde expérimentation  menée sur des données subjectives et physiologiques a également confirmé les liens entre les niveaux de charge de travail, de tension et de vigilance, avec une sous-estimation du niveau physiologique par les novices AT. Les deux groupes comparés (novices AT et conducteurs à la fin du permis probatoire) étaient surchargés en situation complexe, soit car ils n'adoptaient pas les stratégies adéquates (novice), soit par manque de flexibilité par rapport aux automatismes (conducteurs à la fin du permis probatoire). Les résultats établissant les liens entre des caractéristiques de la situation de conduite et des caractéristiques individuelles menant à des accidents de la route pourront permettre d'adapter les actions de sensibilisation et les modules de formations à la conduite.},
   author = {Julie Paxion},
   keywords = {CHARGE MENTALE,COMPLEXITE,CONDUCTEUR EXPERIMENTE,CONDUITE DU VEHICULE,DRIVING EXPERIENCE,DRIVING SIMULATION,EXPERIENCE (HOMME),NOUVEAU CONDUCTEUR,PSYCHOLOGIE,SIMULATEUR (CONDUITE),SIMULATEUR DE CONDUITE,SITUATIONS COMPLEXITY,SYSTEME COMPLEXE,TACHE,TENSION,VIGILANCE,VIGILANCE DU CONDUCTEUR,WORKLOAD},
   month = {12},
   pages = {461 pages},
   publisher = {Aix-Marseille Université},
   title = {Complexité des situations, expérience, tension et vigilance : quels impacts sur la charge de travail et les performances de jeunes conducteurs ?},
   url = {https://hal.science/tel-01214415 https://hal.science/tel-01214415/document},
   year = {2014},
}
@article{Kosch2023,
   abstract = {The ever-increasing number of computing devices around us results in more and more systems competing for our attention, making cognitive workload a crucial factor for the user experience of human-c...},
   author = {Thomas Kosch and Hu Berlin and Germany Jakob Karolus and Johannes Zagermann and Harald Reiterer and Albrecht Schmidt and Paweł W Woźniak and Jakob Karolus and Paweł W Woź and J Zagermann and H Reiterer},
   doi = {10.1145/3582272},
   issn = {15577341},
   issue = {13s},
   journal = {ACM Computing Surveys},
   keywords = {Cognitive workload,categorization,cognition-aware interfaces,physiological sensing,questionnaires,workload assessment,workload-aware computing},
   month = {7},
   publisher = {
ACM
PUB27
New York, NY
},
   title = {A Survey on Measuring Cognitive Workload in Human-Computer Interaction},
   volume = {55},
   url = {https://dl.acm.org/doi/10.1145/3582272},
   year = {2023},
}
@misc{,
   title = {Problems - LeetCode},
   url = {https://leetcode.com/problemset/},
}
@article{Carroll1997,
   abstract = {Human-computer interaction (HCI) study is the region of intersection between psychology and the social sciences, on the one hand, and computer science and technology, on the other. HCI researchers analyze and design specific user interface technologies (e.g. pointing devices). They study and improve the processes of technology development (e.g. task analysis, design rationale). They develop and evaluate new applications of technology (e.g. word processors, digital libraries). Throughout the past two decades, HCI has progressively integrated its scientific concerns with the engineering goal of improving the usability of computer systems and applications, which has resulted in a body of technical knowledge and methodology. HCI continues to provide a challenging test domain for applying and developing psychological and social theory in the context of technology development and use.},
   author = {John M. Carroll},
   doi = {10.1146/ANNUREV.PSYCH.48.1.61},
   issn = {0066-4308},
   journal = {Annual review of psychology},
   keywords = {J M Carroll,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,doi:10.1146/annurev.psych.48.1.61,pmid:15012476},
   pages = {61-83},
   pmid = {15012476},
   publisher = {Annu Rev Psychol},
   title = {Human-computer interaction: psychology as a science of design},
   volume = {48},
   url = {https://pubmed.ncbi.nlm.nih.gov/15012476/},
   year = {1997},
}
@article{Jatupaiboon2013,
   abstract = {In this research we propose to use EEG signal to classify two emotions (i.e., positive and negative) elicited by pictures. With power spectrum features, the accuracy rate of SVM classifier is about 85.41%. Considering each pair of channels and different frequency bands, it shows that frontal pairs of channels give a better result than the other area and high frequency bands give a better result than low frequency bands. Furthermore, we can reduce number of pairs of channels from 7 to 5 with almost the same accuracy and can cut low frequency bands in order to save computation time. All of these are beneficial to the development of emotion classification system using minimal EEG channels in real-time. © 2013 IEEE.},
   author = {Noppadon Jatupaiboon and Setha Pan-Ngum and Pasin Israsena},
   doi = {10.1109/JCSSE.2013.6567313},
   isbn = {9781479908066},
   journal = {Proceedings of the 2013 10th International Joint Conference on Computer Science and Software Engineering, JCSSE 2013},
   keywords = {electroencephalogram,emotion,human computer interaction,support vector machine},
   pages = {21-24},
   title = {Emotion classification using minimal EEG channels and frequency bands},
   year = {2013},
}
@article{Nguyen2022,
   abstract = {GitHub and OpenAI recently launched Copilot, an 'AI pair programmer' that utilizes the power of Natural Language Processing, Static Analysis, Code Synthesis, and Artificial Intelligence. Given a natural language description of the target functionality, Copilot can generate corresponding code in several programming languages. In this paper, we perform an empirical study to evaluate the correctness and understandability of Copilot's suggested code. We use 33 LeetCode questions to create queries for Copilot in four different programming languages. We evaluate the correctness of the corresponding 132 Copilot solutions by running LeetCode's provided tests, and evaluate understandability using SonarQube's cyclomatic complexity and cognitive complexity metrics. We find that Copilot's Java suggestions have the highest correctness score (57%) while JavaScript is the lowest (27%). Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages. We also find some potential Copilot shortcomings, such as generating code that can be further simplified and code that relies on undefined helper methods.},
   author = {Nhan Nguyen and Sarah Nadi},
   doi = {10.1145/3524842.3528470},
   isbn = {9781450393034},
   journal = {Proceedings - 2022 Mining Software Repositories Conference, MSR 2022},
   keywords = {Codex,Empirical Evaluation,GitHub Copilot,Program Synthesis},
   pages = {1-5},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Empirical Evaluation of GitHub Copilot's Code Suggestions},
   url = {https://dl.acm.org/doi/10.1145/3524842.3528470},
   year = {2022},
}
@misc{,
   title = {DORA Community},
   url = {https://dora.community/},
}
@book{Humble2018,
   author = {Jez Humble and Gene Kim},
   isbn = {1942788355},
   publisher = {IT Revolution},
   title = {Accelerate: The science of lean software and devops: Building and scaling high performing technology organizations},
   year = {2018},
}
@inproceedings{Minas2017,
   author = {Randall K Minas and Rick Kazman and Ewan Tempero},
   isbn = {3319586246},
   journal = {Augmented Cognition. Enhancing Cognition and Behavior in Complex Human Environments: 11th International Conference, AC 2017, Held as Part of HCI International 2017, Vancouver, BC, Canada, July 9-14, 2017, Proceedings, Part II 11},
   pages = {56-64},
   publisher = {Springer},
   title = {Neurophysiological impact of software design processes on software developers},
   year = {2017},
}
@article{Fritz2014,
   abstract = {Software developers make programming mistakes that cause serious bugs for their customers. Existing work to detect problematic software focuses mainly on post hoc identification of correlations between bug fixes and code. We propose a new approach to address this problem - - detect when software developers are experiencing difficulty while they work on their programming tasks, and stop them before they can introduce bugs into the code. In this paper, we investigate a novel approach to classify the difficulty of code comprehension tasks using data from psycho-physiological sensors. We present the results of a study we conducted with 15 professional programmers to see how well an eye-tracker, an electrodermal activity sensor, and an electroencephalography sensor could be used to predict whether developers would find a task to be difficult. We can predict nominal task difficulty (easy/difficult) for a new developer with 64.99% precision and 64.58% recall, and for a new task with 84.38% precision and 69.79% recall. We can improve the Naive Bayes classifier's performance if we trained it on just the eye-tracking data over the entire dataset, or by using a sliding window data collection schema with a 55 second time window. Our work brings the community closer to a viable and reliable measure of task difficulty that could power the next generation of programming support tools.},
   author = {Thomas Fritz and Andrew Begel and Sebastian C. Müller and Serap Yigit-Elliott and Manuela Züger},
   doi = {10.1145/2568225.2568266},
   issn = {02705257},
   issue = {1},
   journal = {Proceedings - International Conference on Software Engineering},
   keywords = {psycho-physiological,study,task difficulty},
   month = {5},
   pages = {402-413},
   publisher = {IEEE Computer Society},
   title = {Using psycho-physiological measures to assess task difficulty in software development},
   url = {https://dl.acm.org/doi/10.1145/2568225.2568266},
   year = {2014},
}
@article{Fritz2016,
   abstract = {Producing great software requires great productive developers. Yet, what does it really mean for an individual developer to be productive, and what can we do to best help developers to be productive? To answer these questions, research has traditionally focused on measuring a developer's output and therefore suffered from two drawbacks: the measures can only be calculated after a developer finished her work and these measures do not account for individual differences between developers. The recent advances in biometric sensor technology offer new opportunities to measure a developer's cognitive and emotional states in real-time and thus allow us to know more about what an individual developer is currently experiencing and what might foster or impede the developer's productivity. Results from recent research studies demonstrate the potential that biometric data has to accurately predict aspects of a developer's work, such as perceived task and code difficulty, progress and interruptibility of a developer. This opens up new opportunities for better supporting developers in their work and, for instance, prevent bugs from entering the code, reduce costly interruptions, and foster a better and more productive work day. Our vision is that biometric sensing will be integrated into a developer's work and that biometrics can be used to boost the productivity of each individual developer.},
   author = {Thomas Fritz and Sebastian C. Müller},
   doi = {10.1109/SANER.2016.107},
   isbn = {9781509018550},
   journal = {2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016},
   month = {5},
   pages = {66-77},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Leveraging biometric data to boost software developer productivity},
   volume = {2016-January},
   year = {2016},
}
@article{Crk2015,
   abstract = {Recent decades have seen a resurgence of interest in electroencephalography (EEG), as neuroscience develops new models of cognition and refines old ones, associating them with detectable indicators...},
   author = {Igor Crk and Timothy Kluthe and Andreas Stefik},
   doi = {10.1145/2829945},
   issn = {15577325},
   issue = {1},
   journal = {ACM Transactions on Computer-Human Interaction (TOCHI)},
   keywords = {BCI,HCI,brain-computer interface,empirical studies,human factors,program comprehension},
   month = {12},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Understanding Programming Expertise},
   volume = {23},
   url = {https://dl.acm.org/doi/10.1145/2829945},
   year = {2015},
}
@article{Crk2016,
   abstract = {Empirical studies of programming language learn-ability and usability have thus far depended on indirect measures of human cognitive performance, attempting to capture what is at its essence a purely cognitive exercise through various indicators of comprehension, such as the time spent working out the meaning of code and producing acceptable solutions. We present evidence of the relative contribution of experience and the individual alpha frequency (IAF) to achieving correct performance during program comprehension tasks, specifically that more experience and higher IAF are both associated with an increased likelihood of correct task performance, with experience playing the greater part.},
   author = {Igor Crk and Timothy Kluthe},
   doi = {10.1109/EMBC.2016.7591752},
   isbn = {9781457702204},
   issn = {1557170X},
   journal = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
   month = {10},
   pages = {4601-4604},
   pmid = {28269300},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Assessing the contribution of the individual alpha frequency (IAF) in an EEG-based study of program comprehension},
   volume = {2016-October},
   year = {2016},
}
@article{test,
   abstract = {Generative Artificial Intelligence (GenAI) tools have become increasingly prevalent in software development, offering assistance to various managerial and technical project activities. Notable examples of these tools include OpenAIs ChatGPT, GitHub Copilot, and Amazon CodeWhisperer. Although many recent publications have explored and evaluated the application of GenAI, a comprehensive understanding of the current development, applications, limitations, and open challenges remains unclear to many. Particularly, we do not have an overall picture of the current state of GenAI technology in practical software engineering usage scenarios. We conducted a literature review and focus groups for a duration of five months to develop a research agenda on GenAI for Software Engineering. We identified 78 open Research Questions (RQs) in 11 areas of Software Engineering. Our results show that it is possible to explore the adoption of GenAI in partial automation and support decision-making in all software development activities. While the current literature is skewed toward software implementation, quality assurance and software maintenance, other areas, such as requirements engineering, software design, and software engineering education, would need further research attention. Common considerations when implementing GenAI include industry-level assessment, dependability and accuracy, data accessibility, transparency, and sustainability aspects associated with the technology. GenAI is bringing significant changes to the field of software engineering. Nevertheless, the state of research on the topic still remains immature. We believe that this research agenda holds significance and practical value for informing both researchers and practitioners about current applications and guiding future research.},
   author = {Anh Nguyen-Duc and Beatriz Cabrero-Daniel and Adam Przybylek and Chetan Arora and Dron Khanna and Tomas Herda and Usman Rafiq and Jorge Melegati and Eduardo Guerra and Kai-Kristian Kemell and Mika Saari and Zheying Zhang and Huy Le and Tho Quan and Pekka Abrahamsson},
   isbn = {2310.18648v1},
   keywords = {ChatGPT,CoPilot,Focus Group,GenAI,Generative Artificial Intelligence,Literature review,Literature survey,Research Agenda,Research Roadmap,Software Development,Software Engineering,Software Project,Structured review},
   month = {10},
   title = {Generative Artificial Intelligence for Software Engineering -- A Research Agenda},
   url = {https://arxiv.org/abs/2310.18648v1},
   year = {2023},
}
@article{Nikulin2019,
   abstract = {Innovation projects has established as an emerging field of research, a number of issues have emerged; among the others, measurability and predictability of product development. Moreover, whilst great relevance has been given to aspects such as innovation factors, product results, cognitive knowledge, poor attention has been given to other fundamental factors: product development process and effectiveness of design process as a consequence of complex problem situation. A formalization in affordances of design process may help overcome said issues and enhance product effectiveness, by identifying and highlighting potential designers’ workload perception during the Product Development Process. In this scenario, this paper describes an approach for continuously monitoring and measuring the complexity of design process during the creation of a new product. After an overview of the related work, this study proposes a common instructional design model for Product Development Process, which was split in order to be measured by using NASA-TLX. This research propose that the use of NASA-TLX to predict and measure the designer workload during the design process contributes to a better understanding of the relationships among the different design phases and designer workload. After having shown a case study, a discussion about the possible developments, as well as the limitations of the proposed method, concludes this work.},
   author = {Christopher Nikulin and Gabriela Lopez and Eduardo Piñonez and Luis Gonzalez and Pia Zapata},
   doi = {10.1007/S11423-019-09657-4/TABLES/4},
   issn = {15566501},
   issue = {2},
   journal = {Educational Technology Research and Development},
   keywords = {Design methods,Nasa-TLX,Product development},
   month = {4},
   pages = {467-493},
   publisher = {Springer New York LLC},
   title = {NASA-TLX for predictability and measurability of instructional design models: case study in design methods},
   volume = {67},
   url = {https://link.springer.com/article/10.1007/s11423-019-09657-4},
   year = {2019},
}
@article{McKendricka2018,
   abstract = {The NASA task load index (TLX) is the most used tool for measuring mental workload. The tools widespread use is associated with its pedigree, and its simplicity of application and interpretation. H...},
   author = {Ryan D. McKendricka and Erin Cherry},
   doi = {10.1177/1541931218621010},
   isbn = {9781510889538},
   issn = {10711813},
   journal = {https://doi.org/10.1177/1541931218621010},
   month = {9},
   pages = {44-48},
   publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
   title = {A Deeper Look at the NASA TLX and Where It Falls Short},
   volume = {1},
   url = {https://journals.sagepub.com/doi/abs/10.1177/1541931218621010},
   year = {2018},
}
@article{Hancock2020,
   abstract = {This work considers the future of human interaction with progressively more autonomous systems. I argue that the temporal dissonance between the human’s ‘cycle time’ and machine ‘cycle time,’ will become an overwhelming barrier to collaborative interaction. We may slow machines, we may buffer information exchange, we may default to meta-levels of strategic interchange but in the end all transparency of information interchange will dissolve under the driving influence of time. HF/E is thus already fighting rear-guard action. The question remains as to the sustenance of human quality of life in this evolving milieu.},
   author = {Peter A. Hancock},
   doi = {10.1177/1064804619880047/ASSET/IMAGES/10.1177_1064804619880047-IMG1.PNG},
   issn = {10648046},
   issue = {3},
   journal = {Ergonomics in Design},
   keywords = {artificial intelligence,autonomy,ergonomics,human factors design,machine learning},
   month = {7},
   pages = {4-6},
   publisher = {SAGE Publications Inc.},
   title = {The Humanity of Humanless Systems},
   volume = {28},
   url = {https://journals.sagepub.com/doi/abs/10.1177/1064804619880047},
   year = {2020},
}
@article{Wickens2008,
   abstract = {Objective: The objective is to lay out the rationale for multiple resource theory and the particular 4-D multiple resource model, as well as to show how the model is useful both as a design tool an...},
   author = {Christopher D. Wickens},
   doi = {10.1518/001872008X288394},
   issn = {00187208},
   issue = {3},
   journal = {https://doi.org/10.1518/001872008X288394},
   month = {6},
   pages = {449-455},
   pmid = {18689052},
   publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
   title = {Multiple Resources and Mental Workload},
   volume = {50},
   url = {https://journals.sagepub.com/doi/10.1518/001872008X288394},
   year = {2008},
}
@article{Rubio2004,
   abstract = {The present research evaluates several psychometric properties (intrusiveness, sensitivity, diagnosticity, and validity) of three multidimensional subjective workload assessment instruments: the NASA Task Load Index (TLX), the Subjective Workload Assessment Technique (SWAT), and the Workload Profile (WP). Subjects performed two laboratory tasks separately (single task) and simultaneously (dual task). The results of the ANOVAs performed showed that there are no differences with regard to the three instruments' intrusiveness, and that among the three subjective workload instruments WP has an outstanding sensitivity to the different task manipulations. To evaluate the diagnosticity of each of the three instruments canonical discriminant analysis was used, and this demonstrated that the three multidimensional ratings provided diagnostic information on the nature of tasks demands that was consistent with the a priori task characterisation. However, the diagnostic power of WP was clearly superior to that obtained using TLX or SWAT. Pearson correlations between each performance and each subjective workload measure were calculated to evaluate the concurrent validity of each instrument with task performance, and to assess the convergent validity of the instruments. The three coefficients were positive and near to one, showing the high convergent validity of the three instruments considered in this research. Implementation requirements and subject acceptability were also compared. Finally, practical implications on the three assessment approaches are mentioned.},
   author = {Susana Rubio and Eva Díaz and Jesús Martín and José M. Puente},
   doi = {10.1111/J.1464-0597.2004.00161.X},
   issn = {1464-0597},
   issue = {1},
   journal = {Applied Psychology},
   month = {1},
   pages = {61-86},
   publisher = {John Wiley & Sons, Ltd},
   title = {Evaluation of Subjective Mental Workload: A Comparison of SWAT, NASA-TLX, and Workload Profile Methods},
   volume = {53},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1464-0597.2004.00161.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1464-0597.2004.00161.x https://iaap-journals.onlinelibrary.wiley.com/doi/10.1111/j.1464-0597.2004.00161.x},
   year = {2004},
}

@article{,
   author = {Igor Crk and Timothy Kluthe and Andreas Stefik},
   keywords = {BCI,HCI,brain-computer interface,empirical studies,human factors,program comprehension},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Understanding Programming Expertise},
}

@article{Louis2023,
   abstract = {Mental workload (MWL) is a concept that is used as a reference for assessing the mental cost of activities. In recent times, challenges related to user experience are determining the expected MWL value for a given activity and real-time adaptation of task complexity level to achieve or maintain desired MWL. As a consequence, it is important to have at least one task that can reliably predict the MWL level associated with a given complexity level. In this study, we used several cognitive tasks to meet this need, including the N-Back task, the commonly used reference test in the MWL literature, and the Corsi test. Tasks were adapted to generate different MWL classes measured via NASA-TLX and Workload Profile questionnaires. Our first objective was to identify which tasks had the most distinct MWL classes based on combined statistical methods. Our results indicated that the Corsi test satisfied our first objective, obtaining three distinct MWL classes associated with three complexity levels offering therefore a reliable model (about 80% accuracy) to predicted MWL classes. Our second objective was to achieve or maintain the desired MWL, which entailed the use of an algorithm to adapt the MWL class based on an accurate prediction model. This model needed to be based on an objective and real-time indicator of MWL. For this purpose, we identified different performance criteria for each task. The classification models obtained indicated that only the Corsi test would be a good candidate for this aim (more than 50% accuracy compared to a chance level of 33%) but performances were not sufficient to consider identifying and adapting the MWL class online with sufficient accuracy during a task. Thus, performance indicators require to be complemented by other types of measures like physiological ones. Our study also highlights the limitations of the N-back task in favor of the Corsi test which turned out to be the best candidate to model and predict the MWL among several cognitive tasks.},
   author = {Lina Estelle Linelle Louis and Saïd Moussaoui and Aurélien Van Langhenhove and Sébastien Ravoux and Thomas Le Jan and Vincent Roualdes and Isabelle Milleville-Pennel},
   doi = {10.3389/FPSYG.2023.1122793/BIBTEX},
   issn = {16641078},
   journal = {Frontiers in Psychology},
   keywords = {K-means (KM) clustering,Linear Discriminant Analysis (LDA),NASA-TLX,cognitive tasks,mental workload (MWL),performances,permutation feature importance,workload profile},
   month = {5},
   pages = {1122793},
   publisher = {Frontiers Media S.A.},
   title = {Cognitive tasks and combined statistical methods to evaluate, model, and predict mental workload},
   volume = {14},
   year = {2023},
}
@article{Longo2022,
   abstract = {Human mental workload is arguably the most invoked multidimensional construct in Human Factors and Ergonomics, getting momentum also in Neuroscience and Neuroergonomics. Uncertainties exist in its characterization, motivating the design and development of computational models, thus recently and actively receiving support from the discipline of Computer Science. However, its role in human performance prediction is assured. This work is aimed at providing a synthesis of the current state of the art in human mental workload assessment through considerations, definitions, measurement techniques as well as applications, Findings suggest that, despite an increasing number of associated research works, a single, reliable and generally applicable framework for mental workload research does not yet appear fully established. One reason for this gap is the existence of a wide swath of operational definitions, built upon different theoretical assumptions which are rarely examined collectively. A second reason is that the three main classes of measures, which are self-report, task performance, and physiological indices, have been used in isolation or in pairs, but more rarely in conjunction all together. Multiple definitions complement each another and we propose a novel inclusive definition of mental workload to support the next generation of empirical-based research. Similarly, by comprehensively employing physiological, task-performance, and self-report measures, more robust assessments of mental workload can be achieved.},
   author = {Luca Longo and Christoper D. Wickens and Peter A. Hancock and Gabriela M. Hancock},
   doi = {10.3389/FPSYG.2022.883321/BIBTEX},
   issn = {16641078},
   journal = {Frontiers in Psychology},
   keywords = {definitions,measures,mental workload,models,novel framework,novel inclusive definition,survey,theories},
   month = {6},
   pages = {883321},
   pmid = {35719509},
   publisher = {Frontiers Media S.A.},
   title = {Human Mental Workload: A Survey and a Novel Inclusive Definition},
   volume = {13},
   year = {2022},
}
@article{Ayres2006,
   abstract = {Cognitive load theorists have frequently used subjective measures of cognitive load to test the effectiveness of instructional procedures. This study sought to broaden the applications of subjective measures by testing their ability to detect variations in intrinsic cognitive load within tasks. In two experiments students were asked to complete algebraic problems and provide a subjective measure of cognitive load for each computation completed. By keeping extraneous and germane cognitive load constant, it was argued that changes in intrinsic cognitive load (element interactivity) were measured. Results showed that subjective measures were highly reliable, varied significantly within problems and correlated highly with errors. Evidence was also found that the subjective measures were influenced by the expertise of the learner and procedural errors. © 2006 Elsevier Ltd. All rights reserved.},
   author = {Paul Ayres},
   doi = {10.1016/J.LEARNINSTRUC.2006.09.001},
   issn = {0959-4752},
   issue = {5},
   journal = {Learning and Instruction},
   keywords = {Cognitive load theory,Intrinsic cognitive load,Mathematical errors,Subjective measures},
   month = {10},
   pages = {389-400},
   publisher = {Pergamon},
   title = {Using subjective measures to detect variations of intrinsic cognitive load within problems},
   volume = {16},
   year = {2006},
}
@article{Sewell2016,
   abstract = {Objectives: Few studies have investigated cognitive factors affecting learning of procedural skills in medical education. Cognitive load theory, which focuses on working memory, is highly relevant, but methods for measuring cognitive load during procedural training are not well understood. Using colonoscopy as an exemplar, we used cognitive load theory to develop a self-report instrument to measure three types of cognitive load (intrinsic, extraneous and germane load) and to provide evidence for instrument validity. Methods: We developed the instrument (the Cognitive Load Inventory for Colonoscopy [CLIC]) using a multi-step process. It included 19 items measuring three types of cognitive load, three global rating items and demographics. We then conducted a cross-sectional survey that was administered electronically to 1061 gastroenterology trainees in the USA. Participants completed the CLIC following a colonoscopy. The two study phases (exploratory and confirmatory) each lasted for 10 weeks during the 2014-2015 academic year. Exploratory factor analysis determined the most parsimonious factor structure; confirmatory factor analysis assessed model fit. Composite measures of intrinsic, extraneous and germane load were compared across years of training and with global rating items. Results: A total of 477 (45.0%) invitees participated (116 in the exploratory study and 361 in the confirmatory study) in 154 (95.1%) training programmes. Demographics were similar to national data from the USA. The most parsimonious factor structure included three factors reflecting the three types of cognitive load. Confirmatory factor analysis verified that a three-factor model was the best fit. Intrinsic, extraneous and germane load items had high internal consistency (Cronbach's alpha 0.90, 0.87 and 0.96, respectively) and correlated as expected with year in training and global assessment of cognitive load. Conclusions: The CLIC measures three types of cognitive load during colonoscopy training. Evidence of validity is provided. Although CLIC items relate to colonoscopy, the development process we detail can be used to adapt the instrument for use in other learning settings in medical education.},
   author = {Justin L. Sewell and Christy K. Boscardin and John Q. Young and Olle ten Cate and Patricia S. O'Sullivan},
   doi = {10.1111/MEDU.12965},
   issn = {1365-2923},
   issue = {6},
   journal = {Medical Education},
   month = {6},
   pages = {682-692},
   pmid = {27170086},
   publisher = {John Wiley & Sons, Ltd},
   title = {Measuring cognitive load during procedural skills training with colonoscopy as an exemplar},
   volume = {50},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/medu.12965 https://onlinelibrary.wiley.com/doi/abs/10.1111/medu.12965 https://asmepublications.onlinelibrary.wiley.com/doi/10.1111/medu.12965},
   year = {2016},
}
@article{Couceiro2019,
   abstract = {Our research explores a recent paradigm called Biofeedback Augmented Software Engineering (BASE) that introduces a strong new element in the software development process: The programmers' biofeedback. In this Practical Experience Report we present the results of an experiment to evaluate the possibility of using pupillography to gather biofeedback from the programmers. The idea is to use pupillography to get meta information about the programmers' cognitive and emotional states (stress, attention, mental effort level, cognitive overload,.) during code development to identify conditions that may precipitate programmers making bugs or bugs escaping human attention, and tag the corresponding code locations in the software under development to provide online warnings to the programmer or identify code snippets that will need more intensive testing. The experiments evaluate the use of pupillography as cognitive load predictor, compare the results with the mental effort perceived by programmers using NASATLX, and discuss different possibilities for the use of pupillography as biofeedback sensor in real software development scenarios.},
   author = {Ricardo Couceiro and Goncalo Duarte and Joao Duraes and Joao Castelhano and Catarina Duarte and Cesar Teixeira and Miguel Castelo Branco and Paulo Carvalho and Henrique Madeira},
   doi = {10.1109/DSN.2019.00069},
   isbn = {9781728100562},
   journal = {Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2019},
   keywords = {cognitive overload,human error,mental effort,programmers' biofeedback,pupillography,software faults},
   month = {6},
   pages = {638-644},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Pupillography as Indicator of Programmers' Mental Effort and Cognitive Overload},
   year = {2019},
}
@article{Goncales2019,
   abstract = {Context: In recent years, several studies explored different facets of the developers' cognitive load while executing tasks related to software engineering. Researchers have proposed and assessed different ways to measure developers' cognitive load at work and some studies have evaluated the interplay between developers' cognitive load and other attributes such as productivity and software quality. Problem: However, the body of knowledge about developers' cognitive load measurement is still dispersed. That hinders the effective use of developers' cognitive load measurements by industry practitioners and makes it difficult for researchers to build new scientific knowledge upon existing results. Objective: This work aims to pinpoint gaps providing a classification and a thematic analysis of studies on the measurement of cognitive load in the context of software engineering. Method: We carried out a Systematic Mapping Study (SMS) based on well-established guidelines to investigate nine research questions. In total, 33 articles (out of 2,612) were selected from 11 search engines after a careful filtering process. Results: The main findings are that (1) 55% of the studies adopted electroencephalogram (EEG) technology for monitoring the cognitive load; (2) 51% of the studies applied machine-learning classification algorithms for predicting cognitive load; and (3) 48% of the studies measured cognitive load in the context of programming tasks. Moreover, a taxonomy was derived from the answers of research questions. Conclusion: This SMS highlighted that the precision of machine learning techniques is low for realistic scenarios, despite the combination of a set of features related to developers' cognitive load used on these techniques. Thus, this gap makes the effective integration of the measure of developers' cognitive load in industry still a relevant challenge.},
   author = {Lucian Goncales and Kleinner Farias and Bruno Da Silva and Jonathan Fessler},
   doi = {10.1109/ICPC.2019.00018},
   isbn = {9781728115191},
   journal = {IEEE International Conference on Program Comprehension},
   keywords = {Cognitive Load,Program Comprehension,Software Engineering,Systematic Mapping Study},
   month = {5},
   pages = {42-52},
   publisher = {IEEE Computer Society},
   title = {Measuring the cognitive load of software developers: A systematic mapping study},
   volume = {2019-May},
   year = {2019},
}
@article{Forsgren2021,
   abstract = {Developer productivity is about more than an individual's activity levels or the efficiency of the engineering systems relied on to ship software, and it cannot be measured by a single metric or di...},
   author = {Nicole Forsgren and Margaret Anne Storey and Chandra Maddila and Thomas Zimmermann and Brian Houck and Jenna Butler},
   doi = {10.1145/3454122.3454124},
   issn = {15427749},
   issue = {1},
   journal = {Queue},
   month = {2},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {The SPACE of Developer Productivity},
   volume = {19},
   url = {https://dl.acm.org/doi/10.1145/3454122.3454124},
   year = {2021},
}
@article{Meyer2017,
   abstract = {Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving the productivity of these developers are undertaken without knowledge about how developers spend their time at work. To investigate what activities developers perform during a normal work day and how these activities relate to when developers perceive themselves as productive or unproductive, we deployed a monitoring application at 20 professional software developers' computers at four companies for an average of 11 full work days in situ. Corroborating earlier findings by others, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in fragmented work. Our findings extend beyond existing research results in that we also report on findings that show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning people, low-at-lunch people and afternoon people. A stepwise linear regression per participant revealed that emails, planned meetings and work unrelated web browsing are most often associated with a negative perception of productivity. We discuss opportunities of our findings and suggest design approaches to create tools to better support software developers in planning their work days and improving their personal productivity, e.g., by getting into "the flow".},
   author = {Andre N. Meyer and Laura E. Barton and Gail C. Murphy and Thomas Zimmermann and Thomas Fritz},
   doi = {10.1109/TSE.2017.2656886},
   issn = {0098-5589},
   issue = {12},
   journal = {IEEE Transactions on Software Engineering},
   month = {1},
   pages = {1178-1193},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {The Work Life of Developers: Activities, Switches and Perceived Productivity},
   volume = {43},
   year = {2017},
}
@article{Kosti2018,
   abstract = {This paper provides a proof of concept for the use of wearable technology, and specifically wearable Electroencephalography (EEG), in the field of Empirical Software Engineering. Particularly, we investigated the brain activity of Software Engineers (SEngs) while performing two distinct but related mental tasks: understanding and inspecting code for syntax errors. By comparing the emerging EEG patterns of activity and neural synchrony, we identified brain signatures that are specific to code comprehension. Moreover, using the programmer's rating about the difficulty of each code snippet shown, we identified neural correlates of subjective difficulty during code comprehension. Finally, we attempted to build a model of subjective difficulty based on the recorded brainwave patterns. The reported results show promise towards novel alternatives to programmers’ training and education. Findings of this kind may eventually lead to various technical and methodological improvements in various aspects of software development like programming languages, building platforms for teams, and team working schemes.},
   author = {Makrina Viola Kosti and Kostas Georgiadis and Dimitrios A. Adamos and Nikos Laskaris and Diomidis Spinellis and Lefteris Angelis},
   doi = {10.1016/J.IJHCS.2018.03.002},
   issn = {1071-5819},
   journal = {International Journal of Human-Computer Studies},
   keywords = {Brainwaves,Human factor,Neural synchrony,Neuroergonomics,Software engineering,Wearable EEG},
   month = {7},
   pages = {52-66},
   publisher = {Academic Press},
   title = {Towards an affordable brain computer interface for the assessment of programmers’ mental workload},
   volume = {115},
   year = {2018},
}
@article{Ziegler2022,
   abstract = {Neural code synthesis has reached a point where snippet generation is accurate enough to be considered for integration into human software development workflows. Commercial products aim to increase programmers' productivity, without being able to measure it directly. In this case study, we asked users of GitHub Copilot about its impact on their productivity, and sought to find a reflection of their perception in directly measurable user data. We find that the rate with which shown suggestions are accepted, rather than more specific metrics regarding the persistence of completions in the code over time, drives developers' perception of productivity.},
   author = {Albert Ziegler and Eirini Kalliamvakou and X Alice Li and Andrew Rice and Devon Rifkin and Shawn Simister and Ganesh Sittampalam and Edward Aftandilian and Edward Aftandil},
   doi = {10.1145/3520312.3534864},
   isbn = {9781450392730},
   keywords = {code completion,code synthesis,neural networks,productivity},
   month = {6},
   pages = {21-29},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Productivity assessment of neural code completion},
   url = {https://dl.acm.org/doi/10.1145/3520312.3534864},
   year = {2022},
}
@article{Bird2022,
   abstract = {Over the next five years, AI-powered tools likely will be helping developers in many diverse tasks. For example, such models may be used to improve code review, directing reviewers to parts of a ch...},
   author = {Christian Bird and Denae Ford and Thomas Zimmermann and Nicole Forsgren and Eirini Kalliamvakou and Travis Lowdermilk and Idan Gazit},
   doi = {10.1145/3582083},
   issn = {15427749},
   issue = {6},
   journal = {Queue},
   month = {12},
   pages = {35-57},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Taking Flight with Copilot},
   volume = {20},
   url = {https://dl.acm.org/doi/10.1145/3582083},
   year = {2022},
}
@article{Jiang2023,
   author = {Peiling Jiang and Jude Rayan and Steven P. Dow and Haijun Xia},
   city = {New York, NY, USA},
   doi = {10.1145/3586183.3606737},
   isbn = {9798400701320},
   journal = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
   month = {10},
   pages = {1-20},
   publisher = {ACM},
   title = {Graphologue: Exploring Large Language Model Responses with Interactive Diagrams},
   url = {https://dl.acm.org/doi/10.1145/3586183.3606737},
   year = {2023},
}
@article{Johnson2014,
   abstract = {Coactive Design is a new approach to address the increasingly sophisticated roles that people and robots play as the use of robots expands into new, complex domains. The approach is motivated by th...},
   author = {Matthew Johnson and Jeffrey M. Bradshaw and Paul J. Feltovich and Catholijn M. Jonker and M. Birna Van Riemsdijk and Maarten Sierhuis},
   doi = {10.5898/JHRI.3.1.JOHNSON},
   issue = {1},
   journal = {Journal of Human-Robot Interaction},
   keywords = {coactive design,collaboration,human-agent-robot teamwork,human-robot interaction,interdependence,joint activity,teamwork},
   month = {2},
   pages = {43},
   publisher = {
		Journal of Human-Robot Interaction Steering Committee
		PUB7775
	},
   title = {Coactive design},
   volume = {3},
   url = {https://dl.acm.org/doi/10.5898/JHRI.3.1.Johnson},
   year = {2014},
}
@article{Klein2004,
   author = {Gary Klein and David D. Woods and Jeffrey M. Bradshaw and Robert R. Hoffman and Paul J. Feltovich},
   doi = {10.1109/MIS.2004.74},
   issn = {15411672},
   issue = {6},
   journal = {IEEE Intelligent Systems},
   month = {11},
   pages = {91-95},
   title = {Ten challenges for making automation a "team player" in joint human-agent activity},
   volume = {19},
   year = {2004},
}
@article{Hollnagel2003,
   abstract = {This Handbook serves as a single source for theories, models, and methods related to cognitive task design. It provides the scientific and theoretical basis required by industrial and academic researchers, as well as the practical and methodological guidance needed by practitioners who face problems of building safe and effective human-technology systems. Fundamental across a wide range of disciplines, from military systems to consumer goods and process industries, cognitive task design covers the whole life-cycle of work from pre-analysis, specification, design, risk a. I. Theories -- II. Methods -- III. Field studies.},
   author = {Erik Hollnagel},
   isbn = {9781410607775},
   pages = {808},
   publisher = {Lawrence Erlbaum Publishers},
   title = {Handbook of cognitive task design},
   url = {https://www.google.co.uk/books/edition/Handbook_of_Cognitive_Task_Design/dElPH0ruR-sC?hl=en&gbpv=1&dq=Discovering+how+distributed+cognitive+systems+work.+Handbook+of+cognitive+task+design&pg=PA37&printsec=frontcover},
   year = {2003},
}